

# --- Data product definition template and documentation   --- #

# Data product manifest version. Prefer to have semantic version
version: "0.0.1"

# The meta information regarding the data product. This information will be available for searching and identifying the product within the platform
discoveryPort:
  # "id" will be generated by the meta data service in which this product is registered. It should have a unique identifier/URI in the platform
  id:
  # name might be 
  name: 
  description: 
  # what category does the product belongs to. Eg  retail, finance. ( this wil be different from domain information) 
  category:
  # for 
  image: 
  # The information reagardiong the user. (If the manifest is created by the platfrom userinformation is already avaliable)
  createdBy:
    name: 
    email: 
    userid: 
    url: 
  createTime: 
  # The list of keywords that will help in discovery
  tags: []
  # Will be key value pairs EG: industry: "retail"
  extra: {}
  # domain information of this data product which is registered/created with in the platform
  domainInfo:
    # has to be explored more
    expandme: 

# Input port is a set of channels, by which you will be receiving or pulling data. A product can have multiple input channels active at the same time with any combinations.
inputPorts:
    # The meta information of this specific input port. This information will be registared as 'sources' within the platform
    - alias: 
      description: 
      tags: []
      extra: {}
      # The mechanism which the data for the specific input channel is pulled from the product or pushed through the product. OPTIONS ['pull'|'push']
      syncType: 'pull'
      # each connection will have its own specification to connect to it
      connectionOptions:
        # Type of connection that the product has to establish for communication to the input data source OPTIONS ['postgres'|'mysql'|'vertica'|'s3'|'kafka'|'sftp'|'file',''....]
        type: 'jdbc'
        uri: '${env_vertica_dev_connection_uri}'
        expandme: ''
      # Entity to be pulled from the source
      entity:
        # type will depend on the connectionType, Eg for JDBC it will be table, sftp it will be file type like .csv, .parquet, kafak it will be a topic
        type: 
        entityId: 
        # schema as per the source of an entity. We should store the schema type as it is, supported types are parquet, avro, jsonschema
        sourceSchemaUrl: 
        # expected schema after the projection is applied if there is any . Eg entity schema may be [a,b,c,d] this is the source schema 
        projectSchemaUrl: 
        # Filter query run on the entity , Eg country=uae & email != "mye@mail"
        filter: 
        # will be column name avalable in the schema definitions, Eg country,email,user. There will not be any transformation not even colunm name
        projection: 
        # the date/time the event was logged in the source system
        processingTimeColumn: 
        # the business date/time of the event
        actualTimeColumn: ''
        # columns to use to create a unique key (e.g. hash of columns to be used in scd2 de-duplication)
        uniqueKeyColumns: []
        # Business columns for which to trach changes
        changeTrackColumns: []

    - alias: 
      description: 
      tags: []
      extra: {}
      # If the sync type is 'push' means the data product will open a communication channel and this data product can receive data via that channel
      syncType: 'push'
       # each connection will have its own specification to connect to it
      connectionOptions:
        # The channel that is required to receive the data OPTIONS ['REST','kafkaTopic','S3']
        type: REST
        path: '/${path}'
        # explored more
        expandme: 
      # Entity to be pulled from the source
      entity:
        # type will depend on the connectionType, Eg for JDBC it will be table, sftp it will be file type like .csv, .parquet, kafak it will be a topic
        type: 
        entityId: 
        # schema as per the source of an entity. We should store the schema type as it is, supported types are parquet, avro, jsonschema
        sourceSchemaUrl: 
        # expected schema after the projection is applied if there is any . Eg entity schema may be [a,b,c,d] this is the source schema 
        projectSchemaUrl: 
        # Filter query run on the entity , Eg country=uae & email != "mye@mail"
        filter: 
        # will be column name avalable in the schema definitions, Eg country,email,user. There will not be any transformation not even colunm name
        projection: 
        # the date/time the event was logged in the source system
        processingTimeColumn: 
        # the business date/time of the event
        actualTimeColumn: ''
        # columns to use to create a unique key (e.g. hash of columns to be used in scd2 de-duplication)
        uniqueKeyColumns: []
        # Business columns for which to trach changes
        changeTrackColumns: []
      # an input channel can be another data product also. (The platfrom metadata layer will capture the lineage.)

    - alias:
      isDataProduct: true
      # the product identifier with in the platform
      dataProductId: 
      # how data porduct will consume the linked data product
      accessOptions : {}
      # The list of entities that needs to be pulled from the source
      entities:
        # the output port id of the mentioned data product
        portId: 
        # Filter query run on the entity , Eg country=uae & email != "mye@mail"
        filter: 
        # will be column name avalable in the schema definitions, Eg country,email,user
        projection:
        # expected schema after the projection is applied if there is any . Eg entity schema may be [a,b,c,d] this is the source schema 
        projectSchemaUrl: 
        
# The code that will transform the data from input port and make it avalabe for output port to consume it.
transformation:
  # This can be a local or git project URL
  codeUrl:
  # 
  runtime:
  language:
  libraries:

# Infromation regarding the product data and logical schema. Define the update strategy via updateStrategy options.
stateManagement:
  # the logical schema for the storage of the data product state
  logicalSchemaUrl: 
  # ? histor
  retentionPeriod: 
  # interval of data product has to be refreshed 
  refreshInterval: 
  # data update strategy (appendOnly, scd2/delta, cdc)
  updateStrategy: 

  updateStrategyOptions:
    # the date/time the event was logged in the source system
    processingTimeColumn: 
    # the business date/time of the event
    actualTimeColumn: ''
    # columns to use to create a unique key (e.g. hash of columns to be used in scd2 de-duplication)
    uniqueKeyColumns: []
    # Business columns for which to trach changes
    changeTrackColumns: []
    tags: []
    extra: {}

# Control the way data is consumed from the data product.
outPort:
  - alias: 
    # the type/channel of the output port e.g. SQL, Rest, Cypher, Gremlin, GraphQL, CUBE-OLAP...
    queryType: 
    # slo is a set of metrcis and their corresponding expected/targeted value (e.g. responseTime < 1s )
    slo: {}
    tags: []
    extra: {}

# Define the busineess rules and quality controls, SLOs etc
controlPort:
  dataQualityRules: 
  businessMetrics:
  #
  slo:
  tags: []
  extra: {}

# --- end -- #

# Notes and comments

# how to choose the storage is based on how we allo quries in output port
# [text without limit] --> Vetica[var char (limit)]
# What is present in logicalSchema can be quried and get it via output port
# Eg [name text, age number] -> [name text , age text]
# refreshInterval - product will be executed based on this
